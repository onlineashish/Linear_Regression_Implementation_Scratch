# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
from autograd import grad
from sklearn.linear_model import LinearRegression as LR
import imageio
from os import path
from matplotlib import cm

np.random.seed(42)

def mse_loss_all( coef, X, y, penalty, alpha ):  
  m = len(y)
  theta = coef.T
  h = X @ theta

  #ridge regression 
  if penalty == 'l2':
    J = 1/(2*m) * np.sum((h - y)**2) + alpha/(2*m) * np.sum(theta[1:]**2) 
  #Unregularized mse loss
  elif penalty == 'l1':
    J = 1/(2*m) * np.sum((h - y)**2) + alpha/(2*m) * jax.numpy.abs(theta).sum()
  else:
    J = 1/(2*m) * np.sum((h - y)**2)

  return J

gradient = jax.grad(mse_loss_all, argnums=(0))

class LinearRegression():
  def __init__(self, fit_intercept=True):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    self.X = None
    self.Y = None
    self.X_batch = None
    self.y_batch = None
    
  def fit_sklearn_LR(self, X, y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    model = LR(fit_intercept=True, copy_X=True).fit(X, y)
    self.coef_ = np.hstack((model.intercept_ , model.coef_))
 
  def fit_normal_equations(self, X, y):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    self.X = X
    self.Y = y
    # append a columns of 1s (these are the biases)
    if(self.fit_intercept and "intercept" not in X.columns):
        X.insert(0, "intercept", 1)
    
    X = np.array(X)
    y = np.array(y)

    XT = np.transpose(X)
    XTX_inv = np.linalg.inv(np.matmul(XT,X))
    K = np.matmul(XT,y)
    self.coef_ = np.matmul(XTX_inv,K)

  def fit_SVD(self, X ,y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    # ref: https://sthalles.github.io/svd-for-regression/#:~:text=The%20main%20idea%20of%20the,U%2C%20%CE%A3%2C%20VT..
    self.X = X
    self.Y = y
    # append a columns of 1s (these are the biases)
    if(self.fit_intercept and "intercept" not in X.columns):
        X.insert(0, "intercept", 1)

    U,S,Vt = np.linalg.svd(X, full_matrices=False)
    self.coef_ = Vt.T @ np.linalg.inv(np.diag(S)) @ U.T @ y
    pass

  def mse_loss(self,  X, y, theta ):  
     
    # m = len(y)
    # h = X.dot(theta)

    # #ridge regression 
    # if alpha !=0:
    #   J = 1/(2*m) * np.sum((h - y)**2) + alpha/(2*m) * np.sum(theta[1:]**2) 
    # #Unregularized mse loss
    # else:
    #   J = 1/(2*m) * np.sum((h - y)**2)

    # return J  

    m = y.size
    predictions = X.dot(theta)
    error = predictions - y
    J = (1 / (2 * m)) * np.sum(error ** 2)
    return J
   
  def compute_gradient(self, penalty, alpha):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    grad_f = (self.X_batch.T @ self.y_batch) + self.X_batch.T @ self.X_batch @ self.coef_.T
    grad_f = grad_f / self.X_batch.shape[0]

    return grad_f + alpha * self.coef_.T if penalty == 'l2' else grad_f
  
    

  def compute_jax_gradient(self, penalty, alpha):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    
    return gradient(self.coef_, self.X_batch, self.y_batch, penalty, alpha)
  
    

  def fit_gradient_descent(self, X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    self.X = X
    self.Y = y

    if(self.fit_intercept and "intercept" not in X.columns):
      self.X.insert(0, "intercept", 1)
    
    self.all_coef = pd.DataFrame(columns = np.arange(X.shape[1]))

    self.coef_ = np.random.randn(self.X.shape[1])  

    for epoch in range(1,num_iters+1):
      for batch_start in range(0,X.shape[0],batch_size):
        self.X_batch = np.array(X.iloc[batch_start: min(batch_start+batch_size, X.shape[0]),:])
        self.y_batch = np.array(y[batch_start: min(batch_start+batch_size, X.shape[0])])

        if gradient_type == 'manual':
          self.coef_ -= lr*self.compute_gradient(penalty_type, alpha)
        else:
          self.coef_ -= lr*self.compute_jax_gradient(penalty_type, alpha)
        self.all_coef.loc[len(self.all_coef)]  = self.coef_.flatten()


  def fit_SGD_with_momentum(self, X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0 ,beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    self.X = X
    self.Y = y

    if(self.fit_intercept and "intercept" not in X.columns):
      self.X.insert(0, "intercept", 1)
    
    self.all_coef = pd.DataFrame(columns = np.arange(X.shape[1]))

    self.coef_ = np.random.randn(self.X.shape[1])   
    v = 0
    for epoch in range(1,num_iters+1):
      for batch_start in range(0,X.shape[0],batch_size):
        self.X_batch = np.array(X.iloc[batch_start: min(batch_start+batch_size, X.shape[0]),:])
        self.y_batch = np.array(y[batch_start: min(batch_start+batch_size, X.shape[0])])

        if gradient_type == 'manual':
          v = beta * v + self.compute_gradient(penalty_type, alpha)
          self.coef_ -= lr*v
        else:
          v = beta * v + self.compute_jax_gradient(penalty_type, alpha)
          self.coef_ -= lr*v

        self.all_coef.loc[len(self.all_coef)]  = self.coef_.flatten()


   

  def predict(self, X):
    # Funtion to run the LinearRegression on a test data point
    if(self.fit_intercept and "intercept" not in X.columns):
        X.insert(0, "intercept", 1)
    X = np.array(X)
    y_hat = np.matmul(X,self.coef_)

    return pd.Series(y_hat)


  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    t_0, t_1 = theta_0.values , theta_1.values
    h = 0.08
    x_min, x_max = t_0.min()-10*h , t_0.max() + 10*h
    y_min, y_max = t_1.min()-10*h , t_1.max() +10*h


    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    theta = np.c_[xx.ravel(), yy.ravel()]

    z = [self.mse_loss(self.X,self.Y,theta[i].T) for i in range(len(theta))]
    z = np.array(z).reshape(xx.shape)

    for i in range(len(self.all_coef)):
      fig = plt.figure()
      ax = fig.add_subplot(111, projection ='3d')
      ax.plot_surface(xx,yy,z,cmap=cm.coolwarm, alpha=0.7)
      ax.plot(t_0[i],t_1[i],self.mse_loss(self.X, self.Y,[t_0[i],t_1[i]]), 'ko')

      ax.set_xlabel('theta_0')
      ax.set_ylabel('theta_1')
      ax.view_init(elev=30, azim=45)

      plt.savefig("Image/plt_surface/img"+str(i))
      plt.close(fig)

    frames = []
    for i in range(len(self.all_coef)):
      image = imageio.v2.imread(f'./Image/plt_surface/img{i}.png')
      frames.append(image)


    imageio.mimsave('./Plots/Question3/surface_fit.gif', frames, fps = 10)

    
    pass

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    if not path.exists('Plots/Question3/'):
      os.makedirs('Plots/Question3/')

    x = np.linspace(0,6,100)
    for i in range(len(self.all_coef)):
      fig = plt.figure()
      plt.scatter(X,y, color='orange')
      
      line = theta_0[i] + x*theta_1[i]
      plt.plot(x, line, color='blue')
      plt.xlabel('x')
      plt.ylabel('y')
      plt.ylim((0,26))
      plt.xlim((0,6))
      plt.title("t_0=%.2f   t_1=%.2f" % (theta_0[i],theta_1[i]))
      plt.savefig("Image/plt_line/img"+str(i))
      plt.close(fig)

    frames = []
    for i in range(len(self.all_coef)):
      image = imageio.v2.imread(f'./Image/plt_line/img{i}.png')
      frames.append(image)


    imageio.mimsave('./Plots/Question3/line_fit.gif', frames, fps = 10)      
    pass


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    # theta_0 = theta_0.to_numpy().flatten()
    # theta_1 = theta_1.to_numpy().flatten()
    # x = np.linspace(-1,1,100)
    # y = theta_0 + theta_1*x

    # def cost_func(theta_0, theta_1):
    #   theta_0 = np.atleast_3d(np.asarray(theta_0))
    #   theta_1 = np.atleast_3d(np.asarray(theta_1))
    #   return np.average((y -  (theta_0 + theta_1*x))**2)
    
    # theta0_grid = np.linspace(min(theta_0-1,0),theta_0+1,101)
    # theta1_grid = np.linspace(min(theta_1-2,0),theta_1+2,101)
    # b,m = np.meshgrid(theta0_grid, theta1_grid)
    # zs = np.array([cost_func(bp,mp) for bp,mp in zip(np.ravel(b), np.ravel(m))])
    # Z = zs.reshape(m.shape)

    # for i in range(len(self.all_coef)):s
    #   fig = plt.figure()
    #   ax = fig.add_subplot(111)
    #   ax.contour(b, m, Z)

    #   ax.scatter(theta_0[:i+1],theta_1[:i+1], c='r', s=25, marker='.')
    #   ax.plot(theta_0[:i+1],theta_1[:i+1], c='r')

    #   ax.set_xlabel('theta_0')
    #   ax.set_ylabel('theta_1')

    #   plt.savefig("Images/plt_contr/img"+str(i))
    #   plt.close(fig)


    t_0, t_1 = theta_0.values , theta_1.values
    h = 0.01
    x_min, x_max = t_0.min()-10*h , t_0.max() + 10*h
    y_min, y_max = t_1.min()-10*h , t_1.max() +10*h


    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    theta = np.c_[xx.ravel(), yy.ravel()]

    z = [self.mse_loss(self.X,self.Y,theta[i].T) for i in range(len(theta))]
    z = np.array(z).reshape(xx.shape)

    for i in range(len(self.all_coef)):
      fig = plt.figure()
      ax = fig.add_subplot(111)
      ax.contour(xx, yy, z)

      ax.scatter(t_0[:i+1],t_1[:i+1], c='r', s=25, marker='.')
      ax.plot(t_0[:i+1],t_1[:i+1], c='r')

      ax.set_xlabel('theta_0')
      ax.set_ylabel('theta_1')

      plt.savefig("Image/plt_contr/img"+str(i))
      plt.close(fig)

    frames = []
    for i in range(len(self.all_coef)):
      image = imageio.v2.imread(f'./Image/plt_contr/img{i}.png')
      frames.append(image)


    imageio.mimsave('./Plots/Question3/contr_fit.gif', frames, fps = 10)