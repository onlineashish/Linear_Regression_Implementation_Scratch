# -*- coding: utf-8 -*-
"""Q2_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-h6mR7sy_h7Yi1dKE4xCb8nKprgkVDj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from linearRegression.linear_regression import LinearRegression
from metrics import *
import time

np.random.seed(42)

N = 90
P = 10
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))
print(X.shape)

########  MANUAL->UNREGULARIZED
print('Batch Gradient Descent with manual gradient computation for unregularized objective : ')
LR = LinearRegression(fit_intercept=True)
# Call Gradient Descent here
#X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
batchsize = 9
begin = time.time()
LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='manual',penalty_type=None,num_iters=10,lr=0.01,alpha=0)
end = time.time()
y_hat = LR.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
print('Time taken ',(end-begin))
print("---------------------------")



########  MANUAL->RIDGE 
print('Batch Gradient Descent with manual gradient computation for Ridge regularization objective : ')
LR = LinearRegression(fit_intercept=True)
# Call Gradient Descent here
#X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
batchsize = 9
begin = time.time()
LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='manual',penalty_type='l2',num_iters=10,lr=0.01,alpha=1)
end = time.time()
y_hat = LR.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
print('Time taken ',(end-begin))
print("---------------------------")


########  JAX->UNREGULARIZED
print('Batch Gradient Descent with JAX gradient computation for unregularized objective : ')
LR = LinearRegression(fit_intercept=True)
# Call Gradient Descent here
#X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
batchsize = 9
begin = time.time()
LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='jax',penalty_type=None,num_iters=10,lr=0.01,alpha=0)
end = time.time()
y_hat = LR.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
print('Time taken ',(end-begin))
print("---------------------------")


########  JAX->LASSO
print('Batch Gradient Descent with JAX gradient computation for regularized objective LASSO : ')
LR = LinearRegression(fit_intercept=True)
# Call Gradient Descent here
#X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
batchsize = 9
begin = time.time()
LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='JAX',penalty_type='l1',num_iters=10,lr=0.01,alpha=1)
end = time.time()
y_hat = LR.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
print('Time taken ',(end-begin))
print("---------------------------")


########  JAX->RIDGE
LRate = [0.001,0.01,0.1]
Alpha = [0.1,1,10]
for alpha in Alpha:
    for lr in LRate:
        print('Batch Gradient Descent with JAX gradient computation for RIDGE at lr='+str(lr)+' and lambda='+str(alpha) )
        LR = LinearRegression(fit_intercept=True)
        # Call Gradient Descent here
        #X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
        batchsize = 9
        begin = time.time()
        LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='JAX',penalty_type='l2',num_iters=10,lr=lr,alpha=alpha)
        end = time.time()
        y_hat = LR.predict(X)
        print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
        print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
        print('Time taken ',(end-begin))
        print("---------------------------")


########  JAX->SGD ====> batch size is 1
print('Batch Gradient Descent with JAX gradient computation for regularized objective RIDGE : ')
LR = LinearRegression(fit_intercept=True)
# Call Gradient Descent here
#X, y, batch_size=1, gradient_type="jax", penalty_type="l2", num_iters=20, lr=0.01, alpha = 0):
batchsize = 1
begin = time.time()
LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='JAX',penalty_type='l2',num_iters=1,lr=0.01,alpha=1)
end = time.time()
y_hat = LR.predict(X)
print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
print('Time taken ',(end-begin))
print("---------------------------")


########  JAX->MiniBatch ====> batch size is [10,15,20]
Batchsize = [10,15,20]
for batchsize in Batchsize:
    print('Batch Gradient Descent with JAX gradient computation for regularized objective RIDGE with batch size : ',batchsize)
    LR = LinearRegression(fit_intercept=True)
    begin = time.time()
    LR.fit_gradient_descent(X,y,batch_size=batchsize,gradient_type='JAX',penalty_type='l2',num_iters=10,lr=0.01,alpha=1)
    end = time.time()
    y_hat = LR.predict(X)
    print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
    print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
    print('Time taken ',(end-begin))

    print("---------------------------")


########  SGD MOMENTUM->JAX->RIDGE====> momentum is [0.1, 0.9, 1.5]
# #def fit_SGD_with_momentum(self, X, y, batch_size=1, gradient_type="jax", penalty_type="l2",
#  num_iters=20, lr=0.01, alpha = 0 ,beta=0.9):

Momentum = [0.1, 0.5, 0.9]
for momentum in Momentum:
    batchsize = 9
    print('SGD Momemtum with JAX gradient computation for regularized objective RIDGE with momentum : ',momentum)
    LR = LinearRegression(fit_intercept=True)
    begin = time.time()
    LR.fit_SGD_with_momentum(X,y,batch_size=batchsize,gradient_type='JAX',penalty_type='l2',num_iters=10,lr=0.01,alpha=1,beta =momentum)
    end = time.time()
    y_hat = LR.predict(X)
    print(' Batch size=',batchsize,', RMSE: ', rmse(y_hat, y))
    print(' Batch size=',batchsize,', MAE: ', mae(y_hat, y))
    print('Time taken ',(end-begin))
    
    print("---------------------------")